{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66100c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102\n",
      "1.12.0+cu102\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import liana as li\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "from openproblems import tasks\n",
    "import os \n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Entities\n",
    "from torch_geometric.nn import FastRGCNConv, RGCNConv, GCNConv, InnerProductDecoder, GAE, VGAE\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.io import mmread\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "import anndata\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fe01f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nodes_interactions(matrix):\n",
    "    matrix.index = [str(i).upper() for i in matrix.index.tolist()]\n",
    "\n",
    "\n",
    "\n",
    "    index = matrix.index.tolist()\n",
    "\n",
    "    matrix = matrix.fillna(0)\n",
    "\n",
    "    adata = anndata.AnnData(matrix.transpose())\n",
    "\n",
    "    sc.pp.filter_genes(adata, min_cells=10)\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.leiden(adata)\n",
    "    meta = pd.DataFrame({\"cell\":adata.obs[\"leiden\"].index.tolist(),\"labels\":adata.obs[\"leiden\"].tolist()})\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    meta.index = meta[\"cell\"].tolist()\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    matrix = matrix.loc[adata.var.index.tolist()]\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    #sc.tl.rank_genes_groups(adata, 'labels')\n",
    "    cell_groups = meta['labels'].unique().tolist()\n",
    "    matrix_list = {}\n",
    "    for i in cell_groups:\n",
    "        cells = meta[meta[\"labels\"]==i].index.tolist()\n",
    "        temp_matrix = matrix[cells]\n",
    "        matrix_list[i] = (temp_matrix.mean(axis=1)[temp_matrix.mean(axis=1) > 0].index.tolist())\n",
    "\n",
    "    cell_type_df = matrix_list\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "\n",
    "    #mean_expression = np.mean(matrix.values[matrix.values != 0])\n",
    "    nodes = pd.DataFrame({\"category\":[],\"identifier\":[]})\n",
    "\n",
    "\n",
    "    # In[24]:\n",
    "\n",
    "\n",
    "    LR_nodes = pd.read_csv(\"/h/soemily/GAT/data/LR_database/OmniPath_nodes.csv\",index_col=0)\n",
    "    Omnipath_network = pd.read_csv(\"/h/soemily/GAT/data/LR_database/OmniPath_interactions.csv\",index_col=0)\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "\n",
    "    ligands = LR_nodes[LR_nodes[\"category\"]==\"Ligand\"][\"identifier\"].tolist()\n",
    "    receptors = LR_nodes[LR_nodes[\"category\"]==\"Receptor\"][\"identifier\"].tolist()\n",
    "\n",
    "    ligand_list = []\n",
    "    receptor_list = []\n",
    "    new_cell_df = {}\n",
    "    for i in cell_type_df.keys():\n",
    "        ligand_list.extend(list(set(ligands) & set(cell_type_df[i])))\n",
    "        receptor_list.extend(list(set(receptors) & set(cell_type_df[i])))\n",
    "        new_cell_df[i] = [list(set(ligands) & set(cell_type_df[i])),list(set(receptors) & set(cell_type_df[i]))]\n",
    "\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "\n",
    "    for i in new_cell_df.keys():\n",
    "        new_cell_df[i][0] = [j+\"_Ligand\" for j in new_cell_df[i][0]]\n",
    "        new_cell_df[i][1] = [j+\"_Receptor\" for j in new_cell_df[i][1]]\n",
    "\n",
    "\n",
    "    # In[28]:\n",
    "\n",
    "\n",
    "    ligand_list = list(set(ligand_list))\n",
    "    receptor_list = list(set(receptor_list))\n",
    "\n",
    "\n",
    "    # In[29]:\n",
    "\n",
    "\n",
    "    ligand_list = [i+\"_Ligand\" for i in ligand_list]\n",
    "    receptor_list = [i+\"_Receptor\" for i in receptor_list]\n",
    "\n",
    "\n",
    "    # In[30]:\n",
    "\n",
    "\n",
    "    nodes = pd.concat([nodes,pd.DataFrame({\"category\":[\"Cell Group\"] * len(list(cell_type_df.keys())), \"identifier\":list(cell_type_df.keys())})])\n",
    "    nodes = pd.concat([nodes,pd.DataFrame({\"category\":[\"Ligand\"] * len(ligand_list), \"identifier\":ligand_list})])\n",
    "    nodes = pd.concat([nodes,pd.DataFrame({\"category\":[\"Receptor\"] * len(receptor_list), \"identifier\":receptor_list})])\n",
    "\n",
    "\n",
    "    # In[31]:\n",
    "\n",
    "\n",
    "    new_identifier = [row[\"identifier\"] + \"_\" + row[\"category\"] for index,row in LR_nodes.iterrows()]\n",
    "\n",
    "\n",
    "    # In[32]:\n",
    "\n",
    "\n",
    "    LR_nodes[\"identifier\"] = new_identifier\n",
    "\n",
    "\n",
    "    # In[33]:\n",
    "\n",
    "\n",
    "    nodes[\"Id\"] = range(0,nodes.shape[0])\n",
    "    nodes = nodes[[\"Id\",\"category\",\"identifier\"]]\n",
    "\n",
    "\n",
    "    # In[34]:\n",
    "\n",
    "\n",
    "    nodes.index = nodes.index.astype('int')\n",
    "    nodes[\"Id\"] = nodes[\"Id\"].astype('int')\n",
    "\n",
    "\n",
    "    # In[35]:\n",
    "\n",
    "\n",
    "    meta.index = meta[\"cell\"]\n",
    "\n",
    "\n",
    "    # In[36]:\n",
    "\n",
    "\n",
    "    interactions = pd.DataFrame({\"Src\":[],\"Dst\":[],\"Weight\":[],\"edge_type\":[]})\n",
    "\n",
    "\n",
    "    # In[37]:\n",
    "\n",
    "\n",
    "    LR_nodes.index = LR_nodes[\"Id\"].tolist()\n",
    "\n",
    "\n",
    "    # In[38]:\n",
    "\n",
    "\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"identifier\"].tolist()\n",
    "\n",
    "\n",
    "    # In[39]:\n",
    "\n",
    "\n",
    "    source_list = []\n",
    "    dest_list = []\n",
    "    weight_list = []\n",
    "    edge_type_list = []\n",
    "    for i in new_cell_df.keys():\n",
    "        source_list.extend([i] * (len(new_cell_df[i][0]) + len(new_cell_df[i][1])))\n",
    "        dest_list.extend(new_cell_df[i][0])\n",
    "        dest_list.extend(new_cell_df[i][1])\n",
    "        weight_list.extend([1] * (len(new_cell_df[i][0]) + len(new_cell_df[i][1])))\n",
    "        edge_type_list.extend([1] * (len(new_cell_df[i][0]) + len(new_cell_df[i][1])))\n",
    "\n",
    "\n",
    "    # In[40]:\n",
    "\n",
    "\n",
    "    interactions[\"Src\"] = source_list\n",
    "    interactions[\"Dst\"] = dest_list\n",
    "    interactions[\"Weight\"] = weight_list\n",
    "    interactions[\"edge_type\"] = edge_type_list\n",
    "\n",
    "\n",
    "    # In[41]:\n",
    "\n",
    "\n",
    "    nodes.index = nodes[\"identifier\"].tolist()\n",
    "\n",
    "\n",
    "    # In[42]:\n",
    "\n",
    "\n",
    "    nodes = nodes.drop_duplicates(\"identifier\")\n",
    "    nodes[\"Id\"] = range(0,nodes.shape[0])\n",
    "\n",
    "\n",
    "    # In[43]:\n",
    "\n",
    "\n",
    "    interactions[\"Src\"] = nodes.loc[interactions[\"Src\"].tolist()][\"Id\"].tolist()\n",
    "    interactions[\"Dst\"] = nodes.loc[interactions[\"Dst\"].tolist()][\"Id\"].tolist()\n",
    "\n",
    "    LR_nodes = pd.read_csv(\"/h/soemily/GAT/data/LR_database/OmniPath_nodes.csv\",index_col=0)\n",
    "    Omnipath_network = pd.read_csv(\"/h/soemily/GAT/data/LR_database/OmniPath_interactions.csv\",index_col=0)\n",
    "\n",
    "\n",
    "    # In[55]:\n",
    "\n",
    "\n",
    "    new_identifier = [row[\"identifier\"] + \"_\" + row[\"category\"] for index,row in LR_nodes.iterrows()]\n",
    "\n",
    "\n",
    "    # In[56]:\n",
    "\n",
    "\n",
    "    LR_nodes[\"identifier\"] = new_identifier\n",
    "\n",
    "\n",
    "    # In[57]:\n",
    "\n",
    "\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"identifier\"].tolist()\n",
    "\n",
    "\n",
    "    # In[58]:\n",
    "\n",
    "\n",
    "    LR_nodes = LR_nodes[(LR_nodes[\"identifier\"].isin(ligand_list)) | (LR_nodes[\"identifier\"].isin(receptor_list))]\n",
    "\n",
    "\n",
    "    # In[59]:\n",
    "\n",
    "\n",
    "    Omnipath_network = Omnipath_network[(Omnipath_network[\"Src\"].isin(LR_nodes[\"identifier\"].tolist())) & (Omnipath_network[\"Dst\"].isin(LR_nodes[\"identifier\"].tolist()))]\n",
    "\n",
    "\n",
    "    # In[60]:\n",
    "\n",
    "\n",
    "    LR_nodes[\"Id\"] = range(0,LR_nodes.shape[0])\n",
    "\n",
    "\n",
    "    # In[61]:\n",
    "\n",
    "\n",
    "    LR_nodes.index = LR_nodes[\"identifier\"].tolist()\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"Id\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"Id\"].tolist()\n",
    "\n",
    "\n",
    "    # In[62]:\n",
    "\n",
    "\n",
    "    LR_nodes = LR_nodes[(LR_nodes[\"Id\"].isin(Omnipath_network[\"Src\"].tolist())) | (LR_nodes[\"Id\"].isin(Omnipath_network[\"Dst\"].tolist()))]\n",
    "\n",
    "\n",
    "    # In[63]:\n",
    "\n",
    "\n",
    "    LR_nodes.index = LR_nodes[\"Id\"].tolist()\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"identifier\"].tolist()\n",
    "\n",
    "\n",
    "    # In[64]:\n",
    "\n",
    "\n",
    "    LR_nodes.index = LR_nodes[\"identifier\"].tolist()\n",
    "    LR_nodes[\"Id\"] = range(0,LR_nodes.shape[0])\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"Id\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"Id\"].tolist()\n",
    "\n",
    "    return matrix,meta,nodes,interactions,LR_nodes,Omnipath_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2266e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self,data,num_classes=3):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 3\n",
    "        self.in_head = 3\n",
    "        self.out_head = 3\n",
    "        \n",
    "        \n",
    "        self.conv1 = GATConv(data.x.shape[1], self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head,num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "                \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.logsigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "    \n",
    "\n",
    "\n",
    "# # Get Omnipath embedding\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "import os.path as osp\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# In[66]:\n",
    "\n",
    "\n",
    "def train(model,loader,optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in (loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model()\n",
    "    acc = model.test(z[data.train_mask], data.y[data.train_mask],\n",
    "                     z[data.test_mask], data.y[data.test_mask],\n",
    "                     max_iter=150)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# In[68]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5355bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Omnipath_embeddings(nodes,interactions):\n",
    "    Omnipath_data,Omnipath_nodes,Omnipath_interactions = make_dataset(nodes,interactions,first=False,pathway_encode=False)\n",
    "    node_info = pd.DataFrame(np.zeros((Omnipath_nodes.shape[0],Omnipath_nodes.shape[0])),index=Omnipath_nodes[\"identifier\"].tolist(),columns=Omnipath_nodes[\"identifier\"].tolist())\n",
    "\n",
    "    temp_identifiers = [i.split(\"_\")[0] for i in Omnipath_nodes[\"identifier\"].tolist()]\n",
    "\n",
    "    complexes = pd.read_csv(\"/h/soemily/GAT/data/data/LR_database/complexes.csv\")\n",
    "    complexes = complexes[complexes[\"member\"].isin(temp_identifiers)]\n",
    "\n",
    "    temp_nodes = Omnipath_nodes.copy()\n",
    "    temp_nodes.index = temp_identifiers\n",
    "    temp_nodes = temp_nodes[~temp_nodes.index.duplicated(keep='first')]\n",
    "\n",
    "    complexes[\"member\"] = temp_nodes.loc[complexes[\"member\"].tolist()][\"identifier\"].tolist()\n",
    "\n",
    "    group_complex = complexes.groupby(\"complex\").agg(list)\n",
    "\n",
    "    group_complex.index=range(0,group_complex.shape[0])\n",
    "\n",
    "    for index,row in group_complex.iterrows():\n",
    "        node_info.loc[list(set(row[\"member\"])),list(set(row[\"member\"]))] = index\n",
    "\n",
    "    # for i in group_complex[\"member\"].tolist():\n",
    "    #     node_info.loc[list(set(i)),list(set(i))] = 1\n",
    "\n",
    "    pathways = pd.read_csv(\"/h/soemily/GAT/data/kegg_pathways.csv\",index_col=0)\n",
    "    pathways = pathways[pathways[\"genesymbol\"].isin(temp_identifiers)]\n",
    "    pathways[\"genesymbol\"] = temp_nodes.loc[pathways[\"genesymbol\"].tolist()][\"identifier\"].tolist()\n",
    "    group_pathway = pathways.groupby(\"pathway\").agg(list)\n",
    "\n",
    "    group_pathway.index=range(0,group_pathway.shape[0])\n",
    "\n",
    "    for index,row in group_pathway.iterrows():\n",
    "        node_info.loc[list(set(row[\"genesymbol\"])),list(set(row[\"genesymbol\"]))] += index\n",
    "\n",
    "    # for i in group_pathway[\"genesymbol\"].tolist():\n",
    "    #     node_info.loc[list(set(i)),list(set(i))] += 1\n",
    "\n",
    "    truth_info = pd.DataFrame(np.zeros((Omnipath_nodes.shape[0],Omnipath_nodes.shape[0])),index=Omnipath_nodes[\"identifier\"].tolist(),columns=Omnipath_nodes[\"identifier\"].tolist())\n",
    "\n",
    "    Omnipath_nodes.index = Omnipath_nodes[\"Id\"].tolist()\n",
    "\n",
    "    ident_interactions = Omnipath_interactions.copy()\n",
    "    ident_interactions[\"Src\"] = Omnipath_nodes.loc[ident_interactions[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "    ident_interactions[\"Dst\"] = Omnipath_nodes.loc[ident_interactions[\"Dst\"].tolist()][\"identifier\"].tolist()\n",
    "\n",
    "    for index,row in ident_interactions.iterrows():\n",
    "        truth_info.loc[row[\"Src\"],row[\"Dst\"]] = 1\n",
    "\n",
    "    ligands = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Ligand\"][\"identifier\"].tolist()\n",
    "    receptors = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Receptor\"][\"identifier\"].tolist()\n",
    "    #truth_info = truth_info.loc[ligands,receptors]\n",
    "    truth_info = torch.Tensor(truth_info.values).to(device)\n",
    "\n",
    "\n",
    "    ligands = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Ligand\"][\"Id\"].tolist()\n",
    "    receptors = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Receptor\"][\"Id\"].tolist()\n",
    "\n",
    "\n",
    "    ident_interactions = ident_interactions.drop_duplicates(\"Src\")\n",
    "    ident_interactions = ident_interactions.drop_duplicates(\"Dst\")\n",
    "\n",
    "    ident_interactions.index = range(0,ident_interactions.shape[0])\n",
    "\n",
    "    truth_list = []\n",
    "    for i in Omnipath_nodes[\"identifier\"].tolist():\n",
    "        if \"Ligand\" in i:\n",
    "            if i in ident_interactions[\"Src\"].tolist():\n",
    "                #truth_list.append(ident_interactions[ident_interactions[\"Src\"]==i].index.tolist()[0] + 1)\n",
    "                truth_list.append(1)\n",
    "            else:\n",
    "                truth_list.append(0)\n",
    "        if \"Receptor\" in i:\n",
    "            if i in ident_interactions[\"Dst\"].tolist():\n",
    "                #truth_list.append(ident_interactions[ident_interactions[\"Dst\"]==i].index.tolist()[0] + 1)\n",
    "                truth_list.append(1)\n",
    "            else:\n",
    "                truth_list.append(0)\n",
    "\n",
    "    node_info.values[np.where(np.isnan(node_info.values))] = 0\n",
    "    node_info.values[np.where(np.isinf(node_info.values))] = 0\n",
    "\n",
    "    Omnipath_data.x = torch.Tensor(node_info.values)\n",
    "\n",
    "    Omnipath_data.y = torch.Tensor(truth_list).type(torch.LongTensor)\n",
    "    #Omnipath_data.y = truth_info\n",
    "\n",
    "    Omnipath_nodes.index = Omnipath_nodes[\"Id\"].tolist()\n",
    "\n",
    "    Omnipath_interactions[\"Src\"] = [Omnipath_nodes.loc[i][\"identifier\"] for i in Omnipath_interactions[\"Src\"].tolist()]\n",
    "    Omnipath_interactions[\"Dst\"] = [Omnipath_nodes.loc[i][\"identifier\"] for i in Omnipath_interactions[\"Dst\"].tolist()]\n",
    "\n",
    "    #edge_weights = [max(node_info.loc[i,j],node_info.loc[j,i]) for i,j in zip(Omnipath_interactions[\"Src\"].tolist(),Omnipath_interactions[\"Dst\"].tolist())]\n",
    "    edge_weights = [1 for i,j in zip(Omnipath_interactions[\"Src\"].tolist(),Omnipath_interactions[\"Dst\"].tolist())]\n",
    "    data = Omnipath_data\n",
    "    model = Node2Vec(data.edge_index, embedding_dim=2, walk_length=40,\n",
    "         context_size=40, walks_per_node=10,\n",
    "         num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "\n",
    "    loader = model.loader(batch_size=2, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    \n",
    "    \n",
    "    for epoch in range(1):\n",
    "        loss = train(model,loader,optimizer)\n",
    "        #acc = test()\n",
    "#         if epoch % 10 == 10:\n",
    "#             print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    model.eval()\n",
    "    z = model(torch.arange(data.num_nodes)).detach()\n",
    "\n",
    "    ligand_ids = Omnipath_nodes[Omnipath_nodes[\"category\"].str.contains(\"Ligand\")][\"Id\"].tolist()\n",
    "    receptor_ids = Omnipath_nodes[Omnipath_nodes[\"category\"].str.contains(\"Receptor\")][\"Id\"].tolist()\n",
    "\n",
    "    ligand_embeddings = z[ligand_ids,:]\n",
    "    receptor_embeddings = z[receptor_ids,:]\n",
    "\n",
    "    total_embeddings = torch.inner(ligand_embeddings,receptor_embeddings)\n",
    "\n",
    "    total_embeddings_df = pd.DataFrame(total_embeddings.numpy(),index=Omnipath_nodes[Omnipath_nodes[\"category\"].str.contains(\"Ligand\")][\"identifier\"].tolist(),columns=Omnipath_nodes[Omnipath_nodes[\"category\"].str.contains(\"Receptor\")][\"identifier\"].tolist())\n",
    "    return total_embeddings_df\n",
    "\n",
    "\n",
    "# In[69]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d85264ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "def get_cell_LR_embeddings(matrix,meta,nodes,interactions,total_embeddings_df,Omnipath_nodes,Omnipath_interactions):\n",
    "    device = 'cpu'\n",
    "    truth_info = pd.DataFrame(np.zeros((Omnipath_nodes.shape[0],Omnipath_nodes.shape[0])),index=Omnipath_nodes[\"identifier\"].tolist(),columns=Omnipath_nodes[\"identifier\"].tolist())\n",
    "\n",
    "    #Omnipath_nodes.index = Omnipath_nodes[\"Id\"].tolist()\n",
    "\n",
    "    ident_interactions = Omnipath_interactions.copy()\n",
    "    ident_interactions[\"Src\"] = Omnipath_nodes.loc[ident_interactions[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "    ident_interactions[\"Dst\"] = Omnipath_nodes.loc[ident_interactions[\"Dst\"].tolist()][\"identifier\"].tolist()\n",
    "\n",
    "    for index,row in ident_interactions.iterrows():\n",
    "        truth_info.loc[row[\"Src\"],row[\"Dst\"]] = 1\n",
    "\n",
    "    ligands = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Ligand\"][\"identifier\"].tolist()\n",
    "    receptors = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Receptor\"][\"identifier\"].tolist()\n",
    "    #truth_info = truth_info.loc[ligands,receptors]\n",
    "    truth_info = torch.Tensor(truth_info.values).to(device)\n",
    "\n",
    "    \n",
    "    cell_LR_data,cell_LR_nodes,cell_LR_ints = make_dataset(nodes,interactions,first=False,pathway_encode=False)\n",
    "\n",
    "    full_matrix = pd.DataFrame(np.zeros((cell_LR_nodes.shape[0],cell_LR_nodes.shape[0])),index=cell_LR_nodes[\"identifier\"].tolist(),columns=cell_LR_nodes[\"identifier\"].tolist())\n",
    "\n",
    "    ligands = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Ligand\"][\"Id\"].tolist()\n",
    "    receptors = Omnipath_nodes[Omnipath_nodes[\"category\"]==\"Receptor\"][\"Id\"].tolist()\n",
    "\n",
    "    total_out_df = total_embeddings_df\n",
    "\n",
    "    gene_mean = matrix.mean(axis=1)\n",
    "\n",
    "    Omnipath_nodes.index = [i.split(\"_\")[0] for i in Omnipath_nodes[\"identifier\"].tolist()]\n",
    "\n",
    "    Omnipath_nodes = Omnipath_nodes.loc[~Omnipath_nodes.index.duplicated(),:].copy()\n",
    "    \n",
    "    gene_mean = gene_mean.loc[Omnipath_nodes.index.tolist()]\n",
    "\n",
    "    gene_mean.index = Omnipath_nodes[\"identifier\"].tolist()\n",
    "\n",
    "    ligands = [i for i in gene_mean.index.tolist() if \"Ligand\" in i]\n",
    "    receptors = [i for i in gene_mean.index.tolist() if \"Receptor\" in i]\n",
    "    ligands = list(set(full_matrix.index.tolist()) & set(ligands))\n",
    "    receptors = list(set(full_matrix.index.tolist()) & set(receptors))\n",
    "\n",
    "    for i,j in zip(ligands,receptors):\n",
    "        if (i in gene_mean.index.tolist()) and (j in gene_mean.index.tolist()):\n",
    "            full_matrix.loc[i,j] = gene_mean.loc[i]*gene_mean.loc[j]\n",
    "\n",
    "    ligands = list(set(ligands) & set(total_out_df.index.tolist()))\n",
    "    receptors = list(set(receptors) & set(total_out_df.columns.tolist()))\n",
    "\n",
    "    for i,j in zip(ligands,receptors):\n",
    "        full_matrix.loc[i,j] += total_out_df.loc[i,j]\n",
    "\n",
    "\n",
    "    #full_matrix.loc[true_df.index.tolist(),true_df.columns.tolist()] = true_df.values\n",
    "\n",
    "    cell_groups = meta[\"labels\"].unique().tolist()\n",
    "\n",
    "    ligands = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Ligand\"][\"identifier\"].tolist()\n",
    "    ligands = [i.split(\"_\")[0] for i in ligands]\n",
    "    ligand_matrix = matrix.loc[ligands]\n",
    "\n",
    "    ligand_matrix = ligand_matrix[~ligand_matrix.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "    mean_dict = {}\n",
    "    for i in cell_groups:\n",
    "        cells = meta[meta[\"labels\"]==i][\"cell\"].tolist()\n",
    "        mean_dict[i] = ligand_matrix[cells].mean(axis=1)\n",
    "\n",
    "    full_matrix = full_matrix[~full_matrix.index.duplicated(keep='first')]\n",
    "    full_matrix = full_matrix.loc[:,~full_matrix.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "    for i in mean_dict.keys():\n",
    "        temp_index = [i+\"_Ligand\" for i in mean_dict[i].index.tolist()]\n",
    "        full_matrix.loc[i,temp_index] = mean_dict[i].values\n",
    "        full_matrix.loc[temp_index,i] = mean_dict[i].values\n",
    "\n",
    "    receptors = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Receptor\"][\"identifier\"].tolist()\n",
    "    receptors = [i.split(\"_\")[0] for i in receptors]\n",
    "    receptors_matrix = matrix.loc[receptors]\n",
    "\n",
    "    receptors_matrix = receptors_matrix[~receptors_matrix.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "    mean_dict = {}\n",
    "    for i in cell_groups:\n",
    "        cells = meta[meta[\"labels\"]==i][\"cell\"].tolist()\n",
    "        mean_dict[i] = receptors_matrix[cells].mean(axis=1)\n",
    "\n",
    "    full_matrix = full_matrix[~full_matrix.index.duplicated(keep='first')]\n",
    "    full_matrix = full_matrix.loc[:,~full_matrix.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "    for i in mean_dict.keys():\n",
    "        temp_index = [i+\"_Receptor\" for i in mean_dict[i].index.tolist()]\n",
    "        full_matrix.loc[i,temp_index] = mean_dict[i].values\n",
    "        full_matrix.loc[temp_index,i] = mean_dict[i].values\n",
    "\n",
    "    #full_matrix = (full_matrix - np.min(full_matrix)) / (np.max(full_matrix) - np.min(full_matrix))\n",
    "\n",
    "    full_matrix.values[np.where(np.isnan(full_matrix.values))] = 0\n",
    "    full_matrix.values[np.where(np.isinf(full_matrix.values))] = 0\n",
    "\n",
    "    cell_LR_data.x = torch.Tensor(full_matrix.values)\n",
    "\n",
    "    LR_ids = cell_LR_nodes[(cell_LR_nodes[\"category\"]==\"Ligand\") | (cell_LR_nodes[\"category\"]==\"Receptor\")][\"Id\"].tolist()\n",
    "\n",
    "    # true_values[\"Src\"] = [i + \"_Ligand\" for i in true_values[\"Src\"].tolist()]\n",
    "    # true_values[\"Dst\"] = [i + \"_Receptor\" for i in true_values[\"Dst\"].tolist()]\n",
    "\n",
    "    cell_groups = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Cell Group\"]['identifier'].tolist()\n",
    "\n",
    "    truth_list = []\n",
    "    for i in cell_LR_nodes[\"identifier\"].tolist():\n",
    "        if \"Ligand\" in i:\n",
    "            if i in ident_interactions[\"Src\"].tolist():\n",
    "                #truth_list.append(ident_interactions[ident_interactions[\"Src\"]==i].index.tolist()[0] + 1)\n",
    "                truth_list.append(1)\n",
    "            else:\n",
    "                truth_list.append(0)\n",
    "        elif \"Receptor\" in i:\n",
    "            if i in ident_interactions[\"Dst\"].tolist():\n",
    "                #truth_list.append(ident_interactions[ident_interactions[\"Dst\"]==i].index.tolist()[0] + 1)\n",
    "                truth_list.append(1)\n",
    "            else:\n",
    "                truth_list.append(0)\n",
    "        else:\n",
    "            truth_list.append(2)\n",
    "\n",
    "    cell_LR_data.y = torch.Tensor(truth_list).type(torch.LongTensor)\n",
    "\n",
    "    truth_array = np.array(truth_list)\n",
    "    positive_classes = np.where(truth_array==1)[0].tolist()\n",
    "    negative_classes = np.where(truth_array==0)[0].tolist()[:len(positive_classes)]\n",
    "\n",
    "    new_train_mask = np.array([False]*truth_array.shape[0])\n",
    "    new_train_mask[positive_classes + negative_classes] = True\n",
    "\n",
    "    model = GAT(cell_LR_data,num_classes=2).to(device)\n",
    "    data = cell_LR_data.to(device)\n",
    "    data.train_mask = torch.Tensor(new_train_mask).type(torch.LongTensor)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ligands = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Ligand\"][\"Id\"].tolist()\n",
    "    receptors = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Receptor\"][\"Id\"].tolist()\n",
    "\n",
    "    truth_df = full_matrix.loc[cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Ligand\"][\"identifier\"].tolist(),cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Receptor\"][\"identifier\"].tolist()]\n",
    "\n",
    "    truth_Tensor = torch.Tensor(truth_df.values).to(device)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        #loss = F.nll_loss(out[new_train_mask],data.y[new_train_mask])\n",
    "        ligand_out = out[ligands,:]\n",
    "        receptor_out = out[receptors,:]\n",
    "        total_out = torch.inner(ligand_out,receptor_out)\n",
    "        #loss = criterion(out[LR_ids],data.y)\n",
    "        #loss = criterion(total_out,truth_Tensor)\n",
    "        loss = criterion(out[new_train_mask],data.y[new_train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    cell_LR_out = model(data)\n",
    "\n",
    "    ligands = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Ligand\"][\"Id\"].tolist()\n",
    "    receptors = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Receptor\"][\"Id\"].tolist()\n",
    "\n",
    "    ligand_out = cell_LR_out[ligands,:]\n",
    "    receptor_out = cell_LR_out[receptors,:]\n",
    "    _,ligand_pred = ligand_out.max(dim=1)\n",
    "    _,receptor_pred = receptor_out.max(dim=1)\n",
    "\n",
    "    total_out = torch.inner(ligand_out,receptor_out).cpu().detach().numpy()\n",
    "    \n",
    "    cell_LR_nodes.index = cell_LR_nodes[\"Id\"].tolist()\n",
    "\n",
    "\n",
    "    ligands_df = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Ligand\"]\n",
    "    receptors_df = cell_LR_nodes[cell_LR_nodes[\"category\"]==\"Receptor\"]\n",
    "    ligands_df.index = range(ligands_df.shape[0])\n",
    "    receptors_df.index = range(receptors_df.shape[0])\n",
    "\n",
    "    valid_ligands = ligands_df\n",
    "    valid_receptors = receptors_df\n",
    "    \n",
    "    ligand_pred = ligand_pred.cpu().detach().numpy()\n",
    "    receptor_pred = receptor_pred.cpu().detach().numpy()\n",
    "    \n",
    "    cell_LR_nodes.index = cell_LR_nodes[\"Id\"].tolist()\n",
    "#     valid_ligands = cell_LR_nodes.loc[np.where(ligand_pred == 1)]\n",
    "#     valid_receptors = cell_LR_nodes.loc[np.where(receptor_pred == 1)]\n",
    "#     ligand_out = ligand_out[np.where(ligand_pred ==1)]\n",
    "#     receptor_out = receptor_out[np.where(receptor_pred ==1)]  \n",
    "    total_out = torch.inner(ligand_out,receptor_out).cpu().detach().numpy()\n",
    "    ligand_nodes = cell_LR_nodes[cell_LR_nodes[\"category\"] == \"Ligand\"]\n",
    "    ligand_nodes.index = range(0,ligand_nodes.shape[0])\n",
    "    ligand_idents = ligand_nodes.iloc[np.where(ligand_pred==1)]['identifier'].tolist()\n",
    "    total_out_df = pd.DataFrame(total_out,index=valid_ligands[\"identifier\"].tolist(),columns=valid_receptors[\"identifier\"].tolist())\n",
    "    indicies = np.where(total_out_df.values > 0)\n",
    "    source = list(indicies[0])\n",
    "    dest = list(indicies[1])\n",
    "    index_df = pd.DataFrame({\"Id\":range(0,total_out_df.shape[0]),\"identifier\":total_out_df.index.tolist()})\n",
    "    column_df = pd.DataFrame({\"Id\":range(0,total_out_df.shape[1]),\"identifier\":total_out_df.columns.tolist()})\n",
    "    source_list = index_df.loc[source][\"identifier\"].tolist()\n",
    "    dest_list = column_df.loc[dest][\"identifier\"].tolist()\n",
    "    total_link_df = pd.DataFrame({\"Src\":source_list,\"Dst\":dest_list,\"Prob\":total_out_df.values[indicies]})\n",
    "    total_link_df = total_link_df.sort_values(\"Prob\",ascending=False)\n",
    "    Omnipath_db = pd.read_csv(\"/h/soemily/GAT/data/LR_database/Omnipath_database.csv\")\n",
    "    total_link_df[\"Src\"] = [i.split(\"_\")[0] for i in total_link_df[\"Src\"].tolist()]\n",
    "    total_link_df[\"Dst\"] = [i.split(\"_\")[0] for i in total_link_df[\"Dst\"].tolist()]\n",
    "    total_link_df = total_link_df.drop_duplicates()\n",
    "    return total_link_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ae71b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You’re trying to run this on 14573 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n",
      "WARNING: You’re trying to run this on 14362 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
     ]
    }
   ],
   "source": [
    "matrix = pd.read_csv(\"/h/soemily/GAT/data/Pre_Post/GSE150949/pc9_t7_rep1.csv\",index_col=0)\n",
    "first_matrix,first_meta,first_nodes,first_interactions,first_LR_nodes,first_Omnipath_network = make_nodes_interactions(matrix)\n",
    "first_LR_nodes.index = first_LR_nodes[\"Id\"].tolist()\n",
    "first_Omnipath_network[\"Src\"] = first_LR_nodes.loc[first_Omnipath_network[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "first_Omnipath_network[\"Dst\"] = first_LR_nodes.loc[first_Omnipath_network[\"Dst\"].tolist()][\"identifier\"].tolist()\n",
    "matrix = pd.read_csv(\"/h/soemily/GAT/data/Pre_Post/GSE150949/pc9_t7_rep2.csv\",index_col=0)\n",
    "second_matrix,second_meta,second_nodes,second_interactions,second_LR_nodes,second_Omnipath_network = make_nodes_interactions(matrix)\n",
    "second_LR_nodes.index = second_LR_nodes[\"Id\"].tolist()\n",
    "second_Omnipath_network[\"Src\"] = second_LR_nodes.loc[second_Omnipath_network[\"Src\"].tolist()][\"identifier\"].tolist()\n",
    "second_Omnipath_network[\"Dst\"] = second_LR_nodes.loc[second_Omnipath_network[\"Dst\"].tolist()][\"identifier\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8296c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Preprocessing\n",
      "Done Omnipath\n"
     ]
    }
   ],
   "source": [
    "rep_1_list = []\n",
    "rep_2_list = []\n",
    "for i in range(10):\n",
    "    LR_nodes = pd.concat([first_LR_nodes,second_LR_nodes]).drop_duplicates(\"identifier\")\n",
    "    LR_nodes[\"Id\"] = range(total_LR_nodes.shape[0])\n",
    "    Omnipath_network = pd.concat([first_Omnipath_network,second_Omnipath_network]).drop_duplicates([\"Src\",\"Dst\"])\n",
    "    LR_nodes.index = LR_nodes[\"identifier\"].tolist()\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"Id\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"Id\"].tolist()\n",
    "    Omnipath_network[\"Src\"] = Omnipath_network[\"Src\"].sample(frac=1).tolist()\n",
    "    Omnipath_network[\"Dst\"] = Omnipath_network[\"Dst\"].sample(frac=1).tolist()\n",
    "    print(\"Done Preprocessing\")\n",
    "    df = get_Omnipath_embeddings(LR_nodes,Omnipath_network)\n",
    "    print(\"Done Omnipath\")\n",
    "    LR_nodes.index = LR_nodes[\"identifier\"].tolist()\n",
    "    Omnipath_network[\"Src\"] = LR_nodes.loc[Omnipath_network[\"Src\"].tolist()][\"Id\"].tolist()\n",
    "    Omnipath_network[\"Dst\"] = LR_nodes.loc[Omnipath_network[\"Dst\"].tolist()][\"Id\"].tolist()\n",
    "    Omnipath_data,Omnipath_nodes,Omnipath_interactions = make_dataset(LR_nodes,Omnipath_network,first=False,pathway_encode=False)\n",
    "    Omnipath_nodes.index = Omnipath_nodes[\"identifier\"].tolist()\n",
    "    first_interactions[\"Src\"] = first_interactions[\"Src\"].sample(frac=1).tolist()\n",
    "    first_interactions[\"Dst\"] = first_interactions[\"Dst\"].sample(frac=1).tolist()  \n",
    "    second_interactions[\"Src\"] = second_interactions[\"Src\"].sample(frac=1).tolist()\n",
    "    second_interactions[\"Dst\"] = second_interactions[\"Dst\"].sample(frac=1).tolist()  \n",
    "    first_genes = list(set(first_matrix.index.tolist())& set(Omnipath_nodes.index.tolist()))\n",
    "    second_genes = list(set(second_matrix.index.tolist())& set(Omnipath_nodes.index.tolist()))\n",
    "    first_Omnipath_nodes = Omnipath_nodes.loc[first_genes]\n",
    "    second_Omnipath_nodes = Omnipath_nodes.loc[second_genes]\n",
    "    first_Omnipath_nodes[\"Id\"] = range(first_Omnipath_nodes.shape[0])\n",
    "    second_Omnipath_nodes[\"Id\"] = range(second_Omnipath_nodes.shape[0])\n",
    "    first_Omnipath_nodes.index = first_Omnipath_nodes[\"identifier\"].tolist()\n",
    "    second_Omnipath_nodes.index = second_Omnipath_nodes[\"identifier\"].tolist()\n",
    "    first_Omnipath_interactions = Omnipath_interactions[(Omnipath_interactions[\"Src\"].isin(first_Omnipath_nodes[\"identifier\"].tolist())) & (Omnipath_interactions[\"Dst\"].isin(first_Omnipath_nodes[\"identifier\"].tolist()))]\n",
    "    second_Omnipath_interactions = Omnipath_interactions[(Omnipath_interactions[\"Src\"].isin(second_Omnipath_nodes[\"identifier\"].tolist())) & (Omnipath_interactions[\"Dst\"].isin(second_Omnipath_nodes[\"identifier\"].tolist()))]\n",
    "    rep_1_list.append(get_cell_LR_embeddings(first_matrix,first_meta,first_nodes,first_interactions,df,first_Omnipath_nodes,first_Omnipath_interactions))\n",
    "    print(\"Done Rep 1\")\n",
    "    rep_2_list.append(get_cell_LR_embeddings(second_matrix,second_meta,second_nodes,second_interactions,df,second_Omnipath_nodes,second_Omnipath_interactions))\n",
    "    print(\"Done Rep 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107b096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT kernel",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
